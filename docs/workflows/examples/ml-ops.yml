# MLOps Workflow for Model Training and Deployment
# Handles model retraining, validation, and deployment

name: MLOps Pipeline

on:
  schedule:
    # Run weekly model retraining
    - cron: '0 2 * * 1'  # Monday at 2 AM UTC
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force model retraining'
        required: false
        default: 'false'
        type: boolean
      data_source:
        description: 'Data source for training'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - staging
          - custom
  push:
    paths:
      - 'data/**'
      - 'src/train_model.py'
      - 'src/preprocess_data.py'

env:
  PYTHON_VERSION: "3.12"
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  
jobs:
  # Check if retraining is needed
  check-retraining:
    name: Check Retraining Need
    runs-on: ubuntu-latest
    outputs:
      needs_retraining: ${{ steps.check.outputs.needs_retraining }}
      current_accuracy: ${{ steps.check.outputs.current_accuracy }}
      data_drift_score: ${{ steps.check.outputs.data_drift_score }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Check model performance and data drift
        id: check
        run: |
          python -c "
          import json
          import os
          from src.monitor_performance import check_model_performance
          from src.data_validation import detect_data_drift
          
          # Check current model performance
          try:
              performance = check_model_performance()
              accuracy = performance.get('accuracy', 0.0)
              print(f'Current model accuracy: {accuracy}')
          except Exception as e:
              print(f'Could not check performance: {e}')
              accuracy = 0.0
          
          # Check for data drift
          try:
              drift_score = detect_data_drift()
              print(f'Data drift score: {drift_score}')
          except Exception as e:
              print(f'Could not check data drift: {e}')
              drift_score = 0.0
          
          # Determine if retraining is needed
          needs_retraining = (
              accuracy < 0.8 or 
              drift_score > 0.3 or 
              '${{ inputs.force_retrain }}' == 'true'
          )
          
          print(f'Needs retraining: {needs_retraining}')
          
          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'needs_retraining={str(needs_retraining).lower()}\\n')
              f.write(f'current_accuracy={accuracy}\\n')
              f.write(f'data_drift_score={drift_score}\\n')
          "

  # Data validation and preprocessing
  data-validation:
    name: Data Validation & Preprocessing
    runs-on: ubuntu-latest
    needs: check-retraining
    if: needs.check-retraining.outputs.needs_retraining == 'true'
    outputs:
      data_quality_score: ${{ steps.validate.outputs.data_quality_score }}
      preprocessing_success: ${{ steps.preprocess.outputs.success }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download latest data
        run: |
          # In production, this would fetch data from your data lake/warehouse
          # aws s3 cp s3://data-bucket/latest/ data/raw/ --recursive
          # or fetch from database, API, etc.
          echo "Fetching latest training data..."
          mkdir -p data/raw data/processed

      - name: Validate data quality
        id: validate
        run: |
          python -c "
          import os
          from src.data_validation import validate_data_quality
          
          try:
              quality_score = validate_data_quality('data/raw/customer_data.csv')
              print(f'Data quality score: {quality_score}')
              
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f'data_quality_score={quality_score}\\n')
              
              if quality_score < 0.8:
                  raise Exception(f'Data quality too low: {quality_score}')
                  
          except Exception as e:
              print(f'Data validation failed: {e}')
              exit(1)
          "

      - name: Preprocess data
        id: preprocess
        run: |
          python scripts/run_preprocessing.py
          
          # Check if preprocessing was successful
          if [ -f "data/processed/processed_features.csv" ] && [ -f "data/processed/processed_target.csv" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload processed data
        uses: actions/upload-artifact@v3
        with:
          name: processed-data
          path: |
            data/processed/
            models/preprocessor.joblib

  # Model training with hyperparameter tuning
  model-training:
    name: Model Training
    runs-on: ubuntu-latest
    needs: [check-retraining, data-validation]
    if: needs.data-validation.outputs.preprocessing_success == 'true'
    strategy:
      matrix:
        model_config:
          - { solver: 'liblinear', C: 1.0, penalty: 'l2' }
          - { solver: 'saga', C: 0.5, penalty: 'l1' }
          - { solver: 'lbfgs', C: 2.0, penalty: 'l2' }
    outputs:
      best_model_run_id: ${{ steps.select-best.outputs.run_id }}
      best_model_accuracy: ${{ steps.select-best.outputs.accuracy }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download processed data
        uses: actions/download-artifact@v3
        with:
          name: processed-data
          path: .

      - name: Train model with configuration
        run: |
          python scripts/run_training.py \
            --solver ${{ matrix.model_config.solver }} \
            --C ${{ matrix.model_config.C }} \
            --penalty ${{ matrix.model_config.penalty }} \
            --random_state 42 \
            --experiment_name "automated-training-${{ github.run_id }}"

      - name: Upload model artifacts
        uses: actions/upload-artifact@v3
        with:
          name: model-${{ matrix.model_config.solver }}-${{ matrix.model_config.C }}
          path: |
            models/
            metrics.json

  # Model evaluation and selection
  model-evaluation:
    name: Model Evaluation & Selection
    runs-on: ubuntu-latest
    needs: model-training
    outputs:
      best_model_run_id: ${{ steps.select.outputs.run_id }}
      best_model_accuracy: ${{ steps.select.outputs.accuracy }}
      deployment_approved: ${{ steps.approve.outputs.approved }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download all model artifacts
        uses: actions/download-artifact@v3

      - name: Evaluate and select best model
        id: select
        run: |
          python -c "
          import json
          import os
          import glob
          
          best_accuracy = 0
          best_run_id = None
          
          # Find all metrics files
          for metrics_file in glob.glob('model-*/metrics.json'):
              with open(metrics_file, 'r') as f:
                  metrics = json.load(f)
              
              accuracy = metrics.get('accuracy', 0)
              run_id = metrics.get('run_id', 'unknown')
              
              print(f'Model {run_id}: accuracy = {accuracy}')
              
              if accuracy > best_accuracy:
                  best_accuracy = accuracy
                  best_run_id = run_id
          
          print(f'Best model: {best_run_id} with accuracy {best_accuracy}')
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'run_id={best_run_id}\\n')
              f.write(f'accuracy={best_accuracy}\\n')
          "

      - name: Approve model for deployment
        id: approve
        run: |
          current_accuracy=${{ needs.check-retraining.outputs.current_accuracy }}
          new_accuracy=${{ steps.select.outputs.accuracy }}
          
          # Approve if new model is significantly better
          if (( $(echo "$new_accuracy > $current_accuracy + 0.01" | bc -l) )); then
            echo "approved=true" >> $GITHUB_OUTPUT
            echo "✅ New model approved for deployment (${new_accuracy} > ${current_accuracy})"
          else
            echo "approved=false" >> $GITHUB_OUTPUT
            echo "❌ New model not significantly better (${new_accuracy} vs ${current_accuracy})"
          fi

      - name: Generate model report
        run: |
          echo "# Model Training Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Previous Accuracy:** ${{ needs.check-retraining.outputs.current_accuracy }}" >> $GITHUB_STEP_SUMMARY
          echo "- **New Accuracy:** ${{ steps.select.outputs.accuracy }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Best Run ID:** ${{ steps.select.outputs.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Quality:** ${{ needs.data-validation.outputs.data_quality_score }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Drift:** ${{ needs.check-retraining.outputs.data_drift_score }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployment Approved:** ${{ steps.approve.outputs.approved }}" >> $GITHUB_STEP_SUMMARY

  # Model deployment
  model-deployment:
    name: Model Deployment
    runs-on: ubuntu-latest
    needs: model-evaluation
    if: needs.model-evaluation.outputs.deployment_approved == 'true'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy new model
        run: |
          echo "Deploying model ${{ needs.model-evaluation.outputs.best_model_run_id }}"
          
          # In production, this would:
          # 1. Download model from MLflow
          # 2. Update model in production environment
          # 3. Perform gradual rollout (canary/blue-green)
          # 4. Monitor performance
          
          # Example commands:
          # mlflow models serve -m "models:/${MODEL_NAME}/${MODEL_VERSION}" --port 5000
          # kubectl set image deployment/churn-predictor model=${{ needs.model-evaluation.outputs.best_model_run_id }}

      - name: Verify deployment
        run: |
          # Verify the new model is working correctly
          sleep 30
          
          # Run validation tests
          echo "Running deployment validation..."
          # curl -f http://production-api/health
          # pytest tests/deployment/ --model-version ${{ needs.model-evaluation.outputs.best_model_run_id }}

      - name: Update model registry
        run: |
          # Update model registry with new production model
          echo "Updating model registry..."
          # mlflow models set-tag -n "${MODEL_NAME}" -v "${MODEL_VERSION}" -k "stage" -v "production"

  # Post-deployment monitoring
  post-deployment-monitoring:
    name: Post-Deployment Monitoring
    runs-on: ubuntu-latest
    needs: model-deployment
    
    steps:
      - name: Monitor new model performance
        run: |
          echo "Monitoring new model performance..."
          
          # Monitor for 30 minutes
          for i in {1..6}; do
            echo "Monitoring check $i/6"
            
            # Check model performance metrics
            # accuracy=$(curl -s http://production-api/metrics | grep model_accuracy | awk '{print $2}')
            # error_rate=$(curl -s http://production-api/metrics | grep error_rate | awk '{print $2}')
            
            # if (( $(echo "$accuracy < 0.8" | bc -l) )); then
            #   echo "⚠️ Model performance degraded, consider rollback"
            #   exit 1
            # fi
            
            sleep 300  # 5 minutes
          done
          
          echo "✅ Model monitoring completed successfully"

  # Notification and cleanup
  notify-and-cleanup:
    name: Notify & Cleanup
    runs-on: ubuntu-latest
    needs: [check-retraining, model-evaluation, model-deployment]
    if: always()
    
    steps:
      - name: Notify team
        run: |
          if [ "${{ needs.model-evaluation.outputs.deployment_approved }}" == "true" ]; then
            echo "🚀 New model deployed successfully!"
            echo "Run ID: ${{ needs.model-evaluation.outputs.best_model_run_id }}"
            echo "Accuracy: ${{ needs.model-evaluation.outputs.best_model_accuracy }}"
            # Send Slack/email notification
          elif [ "${{ needs.check-retraining.outputs.needs_retraining }}" == "true" ]; then
            echo "⚠️ Model training completed but not deployed"
            echo "New accuracy not significantly better than current"
          else
            echo "ℹ️ No retraining needed"
            echo "Current model performance is acceptable"
          fi

      - name: Cleanup artifacts
        run: |
          echo "Cleaning up temporary artifacts..."
          # Clean up old model artifacts, temporary data, etc.